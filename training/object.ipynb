{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robbie/.local/lib/python3.8/site-packages/deeplake/util/check_latest_version.py:32: UserWarning: A newer version of deeplake (3.9.5) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n",
      "\\"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening dataset in read-only mode as you don't have write permissions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/activeloop/office-home-domain-adaptation\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hub://activeloop/office-home-domain-adaptation loaded successfully.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://activeloop/office-home-domain-adaptation', read_only=True, tensors=['images', 'domain_categories', 'domain_objects'])\n",
      "\n",
      "      tensor           htype                shape              dtype  compression\n",
      "      -------         -------              -------            -------  ------- \n",
      "      images           image     (15588, 4:6500, 18:6000, 3)   uint8    jpeg   \n",
      " domain_categories  class_label          (15588, 1)           uint32    None   \n",
      "  domain_objects    class_label          (15588, 1)           uint32    None   \n",
      "Length of train_view is 12471\n",
      "Length of val_view is 3117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12471/12471 [00:00<00:00, 16373.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of filtered train dataset: 0\n",
      "Dataset(path='hub://activeloop/office-home-domain-adaptation', read_only=True, index=Index([()]), tensors=['images', 'domain_categories', 'domain_objects'])\n",
      "\n",
      "      tensor           htype              shape            dtype  compression\n",
      "      -------         -------            -------          -------  ------- \n",
      "      images           image     (0, 4:6500, 18:6000, 3)   uint8    jpeg   \n",
      " domain_categories  class_label          (0, 1)           uint32    None   \n",
      "  domain_objects    class_label          (0, 1)           uint32    None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import deeplake\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "import torchvision\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torch\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "import torchvision.models.detection.mask_rcnn\n",
    "\n",
    "# Connect to the training dataset\n",
    "# ds_train = deeplake.load('hub://activeloop/coco-train')\n",
    "\n",
    "# num_classes = len(ds_train.categories.info.class_names)\n",
    "\n",
    "\n",
    "# #dataset hosted by activeloop \n",
    "ds = deeplake.load('hub://activeloop/office-home-domain-adaptation')\n",
    "\n",
    "#summary of dataset tensors \n",
    "ds.summary()\n",
    "\n",
    "ds_train, ds_test = ds.random_split([0.8, 0.2])\n",
    "\n",
    "print(f\"Length of train_view is {len(ds_train)}\")\n",
    "print(f\"Length of val_view is {len(ds_test)}\")\n",
    "\n",
    "\n",
    "##need to implement classes of interest for our dataset https://wandb.ai/istranic/deeplake-demos/reports/Model-Reproducibility-Using-Activeloop-Deep-Lake-and-Weights-Biases--VmlldzoyNzIzNDM1\n",
    "\n",
    "# These are the classes we care about and they will be remapped to 0,1,2,3,4,5,6,7,8 in the model\n",
    "CLASSES_OF_INTEREST = ['Real World']\n",
    "\n",
    "\n",
    "# The classes of interest correspond to the following array values in the current dataset\n",
    "# INDS_OF_INTEREST = [ds_train.labels.info.class_names.index(item) for item in CLASSES_OF_INTEREST]\n",
    "INDS_OF_INTEREST = [ds_train.domain_categories.info.class_names.index(item) for item in CLASSES_OF_INTEREST]\n",
    "\n",
    "\n",
    "# Filter the dataset to only include samples with the specified domain categories\n",
    "ds_filtered_train = ds_train.filter(lambda x: 0 in x['domain_categories'])  # Assuming \"Real World\" is represented by 0\n",
    "# ds_filtered_test = ds_test.filter('domain_categories', CLASSES_OF_INTEREST)\n",
    "\n",
    "# Verify the lengths of the filtered datasets\n",
    "print(f\"Length of filtered train dataset: {len(ds_filtered_train)}\")\n",
    "ds_filtered_train.summary()\n",
    "# print(f\"Length of filtered test dataset: {len(ds_filtered_test)}\")\n",
    "\n",
    "\n",
    "#select only a few objects and one domain \n",
    "#split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation pipeline using Albumentations\n",
    "tform_train = A.Compose([\n",
    "    A.RandomSizedBBoxSafeCrop(width=128, height=128, erosion_rate = 0.2),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(), # transpose_mask = True\n",
    "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels', 'bbox_ids'], min_area=25, min_visibility=0.6)) # 'label_fields' and 'box_ids' are all the fields that will be cut when a bounding box is cut.\n",
    "\n",
    "\n",
    "# Transformation function for pre-processing the Deep Lake sample before sending it to the model\n",
    "def transform(sample_in):\n",
    "\n",
    "    # Convert boxes to Pascal VOC format\n",
    "    boxes = coco_2_pascal(sample_in['boxes'])\n",
    "\n",
    "    # Convert any grayscale images to RGB\n",
    "    images = sample_in['images']\n",
    "    if images.shape[2] == 1:\n",
    "        images = np.repeat(images, int(3/images.shape[2]), axis = 2)\n",
    "\n",
    "    # Pass all data to the Albumentations transformation\n",
    "    # Mask must be converted to a list\n",
    "    masks = sample_in['masks']\n",
    "    mask_shape = masks.shape\n",
    "\n",
    "    # This if-else statement was not necessary in Albumentations <1.3.x, because the empty mask scenario was handled gracefully inside of Albumentations. In Albumebtations >1.3.x, empty list of masks fails\n",
    "    if mask_shape[2]>0:\n",
    "        transformed = tform_train(image = images,\n",
    "                                  masks = [masks[:,:,i].astype(np.uint8) for i in range(mask_shape[2])],\n",
    "                                  bboxes = boxes,\n",
    "                                  bbox_ids = np.arange(boxes.shape[0]),\n",
    "                                  class_labels = sample_in['categories'],\n",
    "                                  )\n",
    "    else:\n",
    "        transformed = tform_train(image = images,\n",
    "                                  bboxes = boxes,\n",
    "                                  bbox_ids = np.arange(boxes.shape[0]),\n",
    "                                  class_labels = sample_in['categories'],\n",
    "                                  )  \n",
    "        \n",
    "\n",
    "\n",
    "    # Convert boxes and labels from lists to torch tensors, because Albumentations does not do that automatically.\n",
    "    # Be very careful with rounding and casting to integers, becuase that can create bounding boxes with invalid dimensions\n",
    "    labels_torch = torch.tensor(transformed['class_labels'], dtype = torch.int64)\n",
    "\n",
    "    boxes_torch = torch.zeros((len(transformed['bboxes']), 4), dtype = torch.int64)\n",
    "    for b, box in enumerate(transformed['bboxes']):\n",
    "        boxes_torch[b,:] = torch.tensor(np.round(box))\n",
    "        \n",
    "\n",
    "    # Filter out the masks that were dropped by filtering of bounding box area and visibility\n",
    "    masks_torch = torch.zeros((len(transformed['bbox_ids']), transformed['image'].shape[1], transformed['image'].shape[2]), dtype = torch.int64)\n",
    "    if len(transformed['bbox_ids'])>0:\n",
    "        masks_torch = torch.tensor(np.stack([transformed['masks'][i] for i in transformed['bbox_ids']], axis = 0), dtype = torch.uint8)\n",
    "    \n",
    "\n",
    "\n",
    "    # Put annotations in a separate object\n",
    "    target = {'masks': masks_torch, 'labels': labels_torch, 'boxes': boxes_torch}\n",
    "\n",
    "    return transformed['image'], target\n",
    "\n",
    "\n",
    "# Conversion script for bounding boxes from coco to Pascal VOC format\n",
    "def coco_2_pascal(boxes):\n",
    "    # Convert bounding boxes to Pascal VOC format and clip bounding boxes to make sure they have non-negative width and height\n",
    "\n",
    "    return np.stack((boxes[:,0], boxes[:,1], boxes[:,0]+np.clip(boxes[:,2], 1, None), boxes[:,1]+np.clip(boxes[:,3], 1, None)), axis = 1)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robbie/.local/lib/python3.8/site-packages/deeplake/integrations/pytorch/common.py:137: UserWarning: Decode method for tensors ['images'] is defaulting to numpy. Please consider specifying a decode_method in .pytorch() that maximizes the data preprocessing speed based on your transformation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "\n",
    "train_loader = ds_train.pytorch(num_workers = 2, shuffle = False, \n",
    "    tensors = ['images', 'masks', 'categories', 'boxes'], # Specify the tensors that are needed, so we don't load unused data\n",
    "    transform = transform, \n",
    "    batch_size = batch_size,\n",
    "    collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for loading the model\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    # Load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # Get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # Get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # Replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robbie/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/robbie/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MaskRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = get_model_instance_segmentation(num_classes)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Specity the optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for training for 1 epoch\n",
    "def train_one_epoch(model, optimizer, data_loader, device):\n",
    "    model.train()\n",
    "\n",
    "    start_time = time.time()\n",
    "    for i, data in enumerate(data_loader):\n",
    "\n",
    "        images = list(image.to(device) for image in data[0])\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in data[1]]\n",
    "        \n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "\n",
    "        # Print performance statistics\n",
    "        batch_time = time.time()\n",
    "        speed = (i+1)/(batch_time-start_time)\n",
    "        print('[%5d] loss: %.3f, speed: %.2f' %\n",
    "              (i, loss_value, speed))\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(f\"Loss is {loss_value}, stopping training\")\n",
    "            print(loss_dict)\n",
    "            break\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        losses.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ Training Epoch 1 ------------------\n",
      "[    0] loss: 5.772, speed: 0.03\n",
      "[    1] loss: 1.770, speed: 0.03\n",
      "[    2] loss: 3.567, speed: 0.02\n",
      "[    3] loss: 3.123, speed: 0.02\n",
      "[    4] loss: 3.593, speed: 0.02\n",
      "[    5] loss: 2.422, speed: 0.02\n",
      "[    6] loss: 1.948, speed: 0.02\n",
      "[    7] loss: 1.555, speed: 0.02\n",
      "[    8] loss: 1.403, speed: 0.02\n",
      "[    9] loss: 1.607, speed: 0.02\n",
      "[   10] loss: 1.476, speed: 0.02\n",
      "[   11] loss: 1.372, speed: 0.02\n",
      "[   12] loss: 1.634, speed: 0.02\n",
      "[   13] loss: 1.471, speed: 0.02\n",
      "[   14] loss: 1.617, speed: 0.02\n",
      "[   15] loss: 1.489, speed: 0.02\n",
      "[   16] loss: 1.728, speed: 0.02\n",
      "[   17] loss: 1.821, speed: 0.02\n",
      "[   18] loss: 2.105, speed: 0.02\n",
      "[   19] loss: 1.657, speed: 0.02\n",
      "[   20] loss: 1.780, speed: 0.02\n",
      "[   21] loss: 1.238, speed: 0.02\n",
      "[   22] loss: 1.549, speed: 0.02\n",
      "[   23] loss: 1.499, speed: 0.02\n",
      "[   24] loss: 1.934, speed: 0.02\n",
      "[   25] loss: 1.281, speed: 0.02\n",
      "[   26] loss: 1.201, speed: 0.02\n",
      "[   27] loss: 1.369, speed: 0.02\n",
      "[   28] loss: 1.998, speed: 0.02\n",
      "[   29] loss: 1.672, speed: 0.02\n",
      "[   30] loss: 1.340, speed: 0.02\n",
      "[   31] loss: 1.705, speed: 0.02\n",
      "[   32] loss: 1.541, speed: 0.02\n",
      "[   33] loss: 1.678, speed: 0.02\n",
      "[   34] loss: 1.514, speed: 0.02\n",
      "[   35] loss: 1.443, speed: 0.02\n",
      "[   36] loss: 1.563, speed: 0.02\n",
      "[   37] loss: 1.293, speed: 0.02\n",
      "[   38] loss: 1.549, speed: 0.02\n",
      "[   39] loss: 1.573, speed: 0.02\n",
      "[   40] loss: 1.414, speed: 0.02\n",
      "[   41] loss: 1.470, speed: 0.02\n",
      "[   42] loss: 1.326, speed: 0.02\n",
      "[   43] loss: 1.687, speed: 0.02\n",
      "[   44] loss: 1.620, speed: 0.02\n",
      "[   45] loss: 1.805, speed: 0.02\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Train the model for 1 epoch\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "    print(\"------------------ Training Epoch {} ------------------\".format(epoch+1))\n",
    "    train_one_epoch(model, optimizer, train_loader, device)\n",
    "    \n",
    "    # --- Insert Testing Code Here ---\n",
    "\n",
    "    print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4robotics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
